# -*- coding: utf-8 -*-
"""PretrainModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cjh8kFkAorrGseNI8wa_2uNe9qYFl47l
"""

!pip install datasets
!pip3 install transformers

from datasets import load_dataset

dataset = load_dataset("multi_nli")

from torchtext.data.utils import get_tokenizer, ngrams_iterator
TOKENIZER = get_tokenizer('basic_english')
import torch
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
import nltk
nltk.download('punkt')

print(dataset['train'][0])

import re 
def cleanData(text):

    clean = text.lower()
    # replace urls
    clean = re.sub(r'http\S+', '<URL>', clean)
    clean = re.sub(r'www\S+', '<URL>', clean)
    # replace hashtags
    clean = re.sub(r'#\w+', '<HASHTAG>', clean)
    # replace mentions
    clean = re.sub(r'@\w+', '<MENTION>', clean)
    # replace _word_ with word
    clean = re.sub(r'_\w+_', lambda x: x.group(0)[1:-1], clean)
    # replace multiple -- with space
    clean = re.sub(r'--+', ' ', clean)

    return clean

from transformers.models.auto.tokenization_auto import TOKENIZER_CONFIG_FILE
from nltk.stem import PorterStemmer
import spacy
import re
nlp = spacy.load('en_core_web_sm')
stop_words = set(stopwords.words('english'))
stemmer=PorterStemmer()

tokens_list = []

def preprocess(data):
    
    sentence = cleanData(data['hypothesis'])
    sentence.lower()
    tokens = nltk.word_tokenize(sentence)
    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]
    tokens_list.append(filtered_tokens)

    stemmed_tokens=[stemmer.stem(token) for token in filtered_tokens]
    lemmatized_tokens=[token.lemma_ for token in nlp(" ".join(stemmed_tokens))]

    # filtered_sentence = ' '.join(filtered_tokens)
    # tokenized = TOKENIZER(filtered_sentence)
    labels = [data['label']]
    return {'tokenised': lemmatized_tokens ,  'label': labels}

train_data = dataset['train'].map(preprocess)

# build vocab
from torchtext.vocab import build_vocab_from_iterator,GloVe
vocab = build_vocab_from_iterator(tokens_list, min_freq=2,specials=['<PAD>','<BOS>','<EOS>','<UNK>'],special_first=True)
vocab.set_default_index(vocab.get_stoi()['<UNK>'])

import numpy as np
glove = GloVe(name='6B', dim=100)
custom_tokens=['<PAD>','<BOS>','<EOS>','<UNK>']
embedding_size = glove.dim
custom_embeddings = torch.rand(len(custom_tokens), embedding_size)
glove.vectors = torch.cat([custom_embeddings, glove.vectors], dim=0)
for token in custom_tokens:
    glove.itos.insert(0, token)
    glove.stoi[token] = 0

data_inp = train_data['tokenised'][:10000]
data_lab = train_data['label'][:10000]

print(data_inp[0])
print(data_lab[0])
# 
# zippoed = zip(list(data_inp), list(data_inp))

# print(len(data_inp))
# print(data_inp[0])
# print(data_lab)

fin_embed = []
max_seq_len = 50

for inp in data_inp:
     
     inp = inp + ['<PAD>'] * (max_seq_len - len(inp))
     embeddings = [glove[token] for token in inp]
  
     print(len(embeddings))
     fin_embed.append(torch.stack(embeddings))
     print(len(fin_embed))

  # stack the embedded sequences into a 3D tensor
fin_embed = torch.stack(fin_embed)

print(len(fin_embed[0]))
temp = fin_embed

import torch
from torch.utils.data import DataLoader

# Create a DataLoader object
batch_size = 50
batches = DataLoader(temp, batch_size=batch_size, shuffle=True)

# Model Architecture

import torch
from torch import nn

class PreTrainedModel(nn.Module):
    def __init__(self, embedding_dim, hidden_dim, vocab_size):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.f1 = nn.LSTMCell(embedding_dim, hidden_dim)
        self.f2 = nn.LSTMCell(hidden_dim, hidden_dim)
        self.b1 = nn.LSTMCell(embedding_dim, hidden_dim)
        self.b2 = nn.LSTMCell(hidden_dim, hidden_dim)
        self.fc = nn.Linear(2*hidden_dim, vocab_size)

    def forward(self, x):
        batch_size, seq_len, _ = x.shape

        # FORWARD LSTM
        fh = fc = [torch.zeros(batch_size, self.hidden_dim, device=x.device) for i in range(2)]
        forward_hidden_states = [fh[1]]
        for t in range(seq_len):
            fh[0], fc[0] = self.f1(x[:, t, :], (fh[0], fc[0]))
            fh[1], fc[1] = self.f2(fh[0], (fh[1], fc[1]))
            forward_hidden_states.append(fh[1])

        # BACKWARD LSTM
        bh = bc = [torch.zeros(batch_size, self.hidden_dim, device=x.device) for i in range(2)]
        backward_hidden_states = [bh[1]]
        for t in reversed(range(seq_len)):
            bh[0], bc[0] = self.b1(x[:, t, :], (bh[0], bc[0]))
            bh[1], bc[1] = self.b2(bh[0], (bh[1], bc[1]))
            backward_hidden_states.append(bh[1])

        final_embeddings = torch.stack([torch.cat((forward_hidden_states[t], backward_hidden_states[t]), dim=1) for t in range(seq_len)])
        prob = self.fc(final_embeddings)

        return prob

epochs = 5
learningrate = 1e-2
embedding_dim= 100
hidden_dim=50
vocab_size=len(vocab)
print("Vocab size: ",vocab_size)

PreModel=PreTrainedModel(embedding_dim,hidden_dim,vocab_size)
optimiser = torch.optim.Adam(PreModel.parameters(), lr=learningrate)
loss_criterion=nn.CrossEntropyLoss()

def train(model,batches,epochs,optimiser,criterion):
     model.train()

     for epoch in range(epochs):
       tloss = 0
       for inp in batches:
           lab = inp.clone()
           optimiser.zero_grad()
           mod_out  =  model(inp)
           #finding loss btwn mod_out and lab
           print(mod_out.size())
           print((mod_out.flatten(end_dim=1)).size())
           loss = loss_criterion(mod_out.view(-1, mod_out.shape[2]), lab.view(-1))
           loss.backward()
       optimiser.step()
       tloss+=loss.item()
     print("Training Loss: ",tloss)

train(PreModel,batches,epochs,optimiser,loss_criterion)

torch.save({
            'model_state_dict': PreModel.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'embedding_dim': embedding_dim,
            'hidden_dim': hidden_dim,
            'vocab_size': vocab_size
            }, 'pretrained_dict.pth')

checkpoint = torch.load('pretrained_dict.pth',map_location="device")
model = PreTrainedModel(embedding_dim, hidden_dim, vocab_size)
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])